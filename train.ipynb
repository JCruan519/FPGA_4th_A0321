{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "import pywt #信号处理库\n",
    "import os\n",
    "from train_funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据的读取\n",
    "data_all_ = np.array(pd.read_csv('original_data.csv'))\n",
    "print(data_all_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看相应库的版本\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过加入噪声来扩充样本\n",
    "data_all = np.zeros((data_all_.shape[0] * 2,data_all_.shape[1]))\n",
    "for i in range(data_all_.shape[0]):\n",
    "    for j in range(data_all_.shape[1] - 1):\n",
    "        add_percent1 = 1 + np.random.uniform(low=-0.1, high=0.1)\n",
    "        add_percent2 = 1 + np.random.uniform(low=-0.1, high=0.1)\n",
    "        add_percent3 = 1 + np.random.uniform(low=-0.1, high=0.1)\n",
    "        add_percent4 = 1 + np.random.uniform(low=-0.1, high=0.1)\n",
    "        add_percent5 = 1 + np.random.uniform(low=-0.1, high=0.1)\n",
    "        data_all[i,j] = data_all_[i,j]\n",
    "        data_all[i,-1] = data_all_[i,-1]\n",
    "        data_all[i+data_all_.shape[0],j] = data_all_[i,j] * add_percent1\n",
    "        data_all[i+data_all_.shape[0],-1] = data_all_[i,-1]\n",
    "\n",
    "print(data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#进行数据的划分\n",
    "data_len = data_all.shape[1] - 1\n",
    "data_sample = data_all.shape[0]\n",
    "\n",
    "import pandas as pd\n",
    "np.random.shuffle(data_all)#数据打乱\n",
    "\n",
    "df1 = pd.DataFrame(data_all[:,:-1])\n",
    "lbls = data_all[:,-1]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def one_hot(y):\n",
    "    lbl = np.zeros(6) #分类个数\n",
    "    lbl[y] = 1\n",
    "    return lbl\n",
    "\n",
    "target = []\n",
    "for value in lbls:\n",
    "    target.append(one_hot(int(value)))\n",
    "target = np.array(target)\n",
    "wave = np.expand_dims(np.array(df1), axis=-1)\n",
    "\n",
    "print(wave.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入LightTCN网络\n",
    "from LightTCN import *\n",
    "\n",
    "model = LightTCN()\n",
    "print(model.summary())#查看网络结构\n",
    "\n",
    "#开始进行训练\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "import os\n",
    "import keras\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练参数的设定\n",
    "X = wave\n",
    "y = target\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 8\n",
    "VALIDATION_SPLIT = 0.3\n",
    "\n",
    "adam = keras.optimizers.Adam()#优化器\n",
    "\n",
    "model.compile(optimizer=adam,\n",
    "              loss=\"categorical_crossentropy\", metrics=[\"acc\"])#模型编译\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    '''\n",
    "    函数作用：每隔25个epoch，学习率减小为原来的0.5\n",
    "    输入：epoch轮次\n",
    "    输出：改变后的学习率\n",
    "    '''\n",
    "    if epoch % 25 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.5)\n",
    "        print(\"lr changed to {}\".format(lr * 0.5))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "lrate = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# filepath=\"model/{epoch:02d}-{val_acc:.2f}.h5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,\n",
    "# mode='max')\n",
    "# callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型的训练与保存\n",
    "history = model.fit(X, y, epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT,\n",
    "                    verbose=1, callbacks=[lrate])\n",
    "save_path = 'model/model.h5'\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#绘制acc和loss曲线\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "# plt.figure(figsize=(10,5),dpi=300)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.savefig('acc.png')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "# plt.figure(figsize=(10,5),dpi=300)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.savefig('loss.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
